## 学习深度学习是否要先学习机器学习？

这个问题太经典了。

这是一个典型的学生思维。

我先说结论：为了做你那个神经网络的毕设，你不需要系统地、从头到尾地把机器学习的课程全部学一遍。但是，你也绝对不能对机器学习的核心思想一无所知。

至于为什么，现在我分几点展开讲讲。

#### 一、 现在的行业现状到底是个什么鬼样子

我们先对齐一下时间线。现在是2025年末。几年前我们面试算法工程师，还会问你GBDT的原理，问你XGBoost怎么做并行优化，问你CRF怎么推导。

现在呢？现在的面试画风完全变了。

我们更关心你对Transformer架构的理解，关心你对LoRA微调的认知，关心你能不能搞定RAG也就是检索增强生成的全链路问题。

在2025年的今天，如果你直接上手搞深度学习，你会发现门槛极低。Pytorch哪怕到现在这个版本，写个神经网络也就是几十行代码的事。HuggingFace上的模型拉下来，几行代码就能跑通一个原本需要几百人团队才能搞定的NLP任务。

这种低门槛恰恰是最大的陷阱。

很多现在的实习生，进来就能跑通大模型，就能做微调。但是一旦模型效果不好，损失函数Loss不下降，或者模型在训练集上表现神勇但在测试集上像个智障一样，他们就懵了。他们不知道怎么调参，不知道什么是偏差和方差的权衡，不知道数据分布不一致该怎么处理。

这个时候，能救你的，全都是传统机器学习里讲烂了的概念。

所以我的建议是：并行推进，以战养战。

绝不要拿出一两个月的时间专门死磕统计学习方法或者西瓜书，那是最劝退的。你要做的是，在做你那个神经网络毕设的过程中，遇到不懂的概念，回过头去查机器学习的资料。

#### 二、 什么是你要警惕的学生思维

典型的学生思维就是：线性学习。我觉得我要学C，必须先学A和B。

我要做神经网络，必须先把线性回归、逻辑回归、支持向量机、决策树、随机森林、贝叶斯分类器全部学通透。这种思维在学校里没错，因为知识体系需要构建。但是在工程界，特别是在IT互联网这种技术迭代快得像投胎一样的行业，这种思维是致命的。

你在做毕设，目的是解决问题，或者说完成项目。

真实的工程场景是：老板让你做一个垃圾邮件分类系统。你不需要从头去学贝叶斯定理的证明，你只需要知道贝叶斯适合做文本分类，然后调包，跑通，看效果。效果不好，再去研究原理。如果你现在从机器学习开始学，你大概率会卡在SVM的对偶问题和核函数上，然后你会卡在HMM的那些复杂概率计算上。等你头昏脑涨地搞懂了这些，你会发现，这玩意儿在深度学习里压根用不上。现在的深度学习，核心是梯度下降，是反向传播。SVM那一套优美的数学理论，在暴力美学的神经网络面前，确实有点过时了。所以，别为了感动自己而去啃那些硬骨头。

#### 三、 机器学习里，哪些是你必须掌握的“硬通货”

虽然我不建议你系统性地全学，但有几个机器学习的核心概念，是你做深度学习绝对绕不开的。

如果你不懂这些，你做神经网络就像盲人摸象。

##### 1. 线性回归与逻辑回归

这是所有模型的祖宗。你做的神经网络，拆开来看，每一个神经元其实就是一个线性模型加上一个激活函数。如果你不理解线性回归里的权重和偏置是什么物理含义，你就无法理解神经网络在学什么。如果你不理解逻辑回归里的Sigmoid函数是怎么把一个数值映射成概率的，你就看不懂神经网络最后的输出层。

##### 2. 过拟合与欠拟合

这是AI领域永恒的主题，不管你是做2025年的万亿参数大模型，还是做2010年的小数据分类。

你需要深刻理解：为什么模型在训练集上准确率100%，到了测试集就只有50%？这就是过拟合。

机器学习里解决过拟合的方法：正则化。L1正则，L2正则。这些概念在深度学习里直接对应：Dropout，Weight Decay。如果你在机器学习里没搞懂L2正则的本质是限制权重的大小，那你在深度学习里调参的时候就是在瞎蒙。

##### 3. 评估指标

你做完毕设，怎么证明你的模型好？只看准确率？如果你是做癌症预测，1000个人里只有1个癌症，你的模型全部预测健康，准确率也是99.9%，但这个模型是垃圾。你需要懂：精确率、召回率、F1-Score、AUC/ROC曲线。这些全是机器学习的基础课。不管模型多先进，评估标准是通用的。

#### 四、 哪些机器学习的内容是可以战略性放弃的

为了你的毕设，也为了你在这个内卷的时代活下来，你需要做减法。

##### 1. 复杂的数学推导

除非你是要发顶会Paper，否则别去推导SVM的拉格朗日对偶性。别去推导GMM的EM算法收敛性。现在的深度学习框架已经把自动求导封装得太好了。你只需要理解原理，不需要当数学家。

##### 2. 传统降维算法

PCA了解一下概念就行。现在的深度学习里，我们用AutoEncoder，用Embedding层来做降维和特征提取，比PCA强大得多。

#### 五、 2025年视角下的新建议

你现在处于2025年末，这个时间节点非常特殊。AI已经从炼丹变成了搭积木，甚至变成了Prompt Engineering。

但是，作为一个要混这行的人，你不能只甘心做一个API调用师。

我给你规划一个针对你毕设的、极其功利的学习路径：

##### 阶段一：快速上手（耗时：1周）

直接上手深度学习。不要管原理，先跑通代码。你的毕设是神经网络，那就先找一个类似的开源项目，GitHub是最好的老师。

目标： 让代码跑起来，不报错，能出结果。在这个阶段，你会遇到一堆不懂的名词：Batch Size, Learning Rate, Optimizer, Epoch。遇到一个查一个。不要系统学，要碎片化学。

##### 阶段二：恶补原理（耗时：穿插在整个过程）

当你发现模型效果不好，或者你想改模型结构的时候，瓶颈就来了。这时候，回去翻机器学习的书。比如，你发现模型收敛太慢。你去查资料，发现需要用Adam优化器。为什么是Adam？因为它结合了动量。什么是动量？这就要回到机器学习的基础优化理论。这种倒逼式的学习，效率最高，记忆最深。

##### 阶段三：数据为王（耗时：占你精力的50%）

不管是机器学习还是深度学习，甚至是现在的GPT-5，核心永远是数据。你在知乎上看到的大神，从来不会告诉你，他们80%的时间其实是在洗数据。你需要掌握pandas，numpy这些库。但是别只看官方文档，那个太枯燥。我建议你去找一些大厂的实战手册看看，比如字节内部Pandas操作手册.pdf，它把常见的数据需求都拆成了清晰的场景，像用户留存分析、日志时间序列处理这些。这种基于业务场景的学习，比你死记硬背函数强一万倍。归一化、标准化、One-Hot编码，这些机器学习里的基础数据处理操作，在深度学习里一个都不能少。

#### 六、 一个真实的翻车案例

为了让你印象深刻，我讲一个我带过的实习生的真事。

这孩子名校毕业，理论基础很扎实，一来就跟我说要搞个基于Transformer的复杂时序预测模型来预测我们服务器的流量负载。他搞了两个月，模型结构改得花里胡哨，又是加Attention又是加Residual Connection，显卡烧得呼呼转。结果呢？预测准确率一直上不去，MSE很高。我看了一眼他的数据，发现他根本没做去噪和平稳性检测。而且，对于这种简单的时间序列，有时候一个简单的线性回归或者ARIMA模型，效果可能比Transformer还好，而且训练只需要几秒钟。最后我让他先用机器学习里的随机森林跑了个基准。结果随机森林的准确率直接吊打了他那个搞了两个月的Transformer。为什么？因为数据量不够大。

深度学习是数据饥渴型的，数据量少的时候，机器学习算法往往更稳健，更不容易过拟合。

这个案例告诉你：学习机器学习，不是为了让你在所有地方都用它，而是为了让你有一个Baseline，有一个参照系。当你做神经网络毕设的时候，如果你的神经网络跑不过逻辑回归，那你就要反思是你模型设计得太烂，还是数据本身就没有规律。既然要收藏，我就得给点真东西。但我不推荐那些大部头，在2025年，看书太慢了。

##### 1. 快速概念扫盲StatQuest with Josh Starmer。

这个必须强推。这哥们用极其生动的动画，把复杂的机器学习概念讲得连小学生都能听懂。这里就是你遇到不懂的概念时的字典。

##### 2. 补齐数学短板

如果你觉得数学实在是你的死穴，看公式就头疼，那别去看同济版高数了。你可以看看鸢尾花书.pdf这种可视化数学丛书。它不像传统教材那样堆砌公式，而是用了海量的图解，把抽象的数学概念画给你看。对于工程师来说，建立直观理解比会推导公式重要得多。

##### 3. 深度学习与ML的衔接

Fast.ai。Jeremy Howard的课程理念就是：先让你跑通模型，再回头讲原理。非常适合你的“学生思维”矫正。

他的课程里会穿插很多机器学习的直觉，告诉你为什么这里要这样处理数据，那里要那样初始化权重。

#### 八、 写在最后

回到你的毕设。你想做神经网络，没问题，直接去做。不要因为觉得“我机器学习还没学完”就产生畏难情绪。

在这个行业里，没有人是学完了再上岗的。如果你等到什么都懂了再动手，那只说明一件事：你学的那个东西已经过时了。在IT互联网摸爬滚打这么多年，我见过最厉害的工程师，不是那些背书背得最溜的，而是那些直觉最好的。这种直觉，就是通过不断地掉坑、爬坑练出来的。机器学习的知识，就是你爬坑时候的梯子。你不需要先把梯子造得完美无缺，你只需要知道哪里有梯子，什么时候该用梯子。

所以，放手去搞你的神经网络。遇到不懂的梯度消失，去查；遇到不懂的正则化，去查；遇到不懂的交叉熵，去查。在这个过程中积累下来的机器学习知识，才是真正属于你的，才是能让你在面试官面前侃侃而谈的资本。




