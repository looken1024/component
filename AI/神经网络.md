## 如何防止过拟合？

在机器学习中，**过拟合**是指模型在训练数据上表现非常好，但在未见过的测试数据上表现很差的现象。这意味着模型过度学习了训练数据中的噪声和细节，导致泛化能力差。

防止过拟合是提升模型泛化能力的关键。以下是一些常用且有效的方法，从数据、模型、训练过程和集成学习等角度进行分类：

#### 一、 数据层面

这是最根本、最有效的方法之一。

- 获取更多数据：更多的数据能让模型学习到更本质的分布规律，而不是记住噪声。这是最理想但有时最难实现的方法。
- 数据增强：在数据量有限时，通过对现有数据进行人工变换来创造“新”数据。
    - 图像：旋转、翻转、裁剪、缩放、调整亮度对比度、添加噪声等。
    - 文本：同义词替换、回译、随机删除或交换词语等。
    - 音频：变速、变调、添加背景噪声等。
- 特征工程
    - 特征选择：移除不相关或冗余的特征，降低模型复杂度，减少噪声。方法有：过滤法（如相关系数）、包裹法（如递归特征消除）、嵌入法（如L1正则化）。
    - 降维：使用主成分分析（PCA）、线性判别分析（LDA）或t-SNE等方法减少特征数量，保留最重要的信息。

#### 二、 模型层面

降低模型本身的复杂度，使其更倾向于学习简单的模式。

- 选择更简单的模型：在满足需求的前提下，优先选择参数少、结构简单的模型（如线性模型而非深度神经网络）。
- 减少模型容量：
    - 神经网络：减少网络层数、每层的神经元（宽度）数量。
    - 决策树：降低树的最大深度、增加分裂所需的最小样本数、增加叶节点的最小样本数。
- 正则化：在损失函数中添加一个惩罚项，直接限制模型参数的大小，迫使模型权重趋向于小值，从而得到更平滑的决策边界。
    - L1正则化（Lasso）：惩罚项是权重的绝对值之和。倾向于产生稀疏权重矩阵，即让许多权重变为0，天然具有特征选择功能。
    - L2正则化（Ridge）：惩罚项是权重的平方和。倾向于让权重均匀地变小，但不会为0。
    - 弹性网络：L1和L2正则化的结合。
    - 在神经网络中，L2正则化通常通过设置优化器的 weight_decay 参数实现。

#### 三、 训练过程层面

在训练过程中加入一些技术来防止过拟合。

- 提前停止：最实用、最受欢迎的方法之一。
    - 将数据分为训练集、验证集和测试集。
    - 在训练集上训练，同时监控模型在验证集上的性能（如损失或准确率）。
    - 当验证集性能不再提升甚至开始下降时（即模型开始过拟合训练数据），立即停止训练，并回滚到验证集性能最好的那个模型状态。
- Dropout（主要用于神经网络）：
    - 在训练过程中，以一定的概率 p 随机“丢弃”（即暂时从网络中移除）一部分神经元及其连接。
    - 这迫使网络不能过度依赖任何少数神经元，必须学习到更鲁棒、更分散的特征，可以看作是一种对多个子网络进行集成学习的近似。
    - 在测试/预测阶段，不使用Dropout，并且所有神经元的输出需要乘以 (1-p) 以保持期望值一致。
- 批量归一化：虽然其主要目的是加速训练和缓解梯度消失/爆炸，但通过规范化每层的输入分布，也具有一定的正则化效果，能轻微帮助防止过拟合。

#### 四、 集成学习方法

通过结合多个模型来降低方差，提高泛化能力。

- Bagging：通过自助采样法训练多个基学习器，然后进行平均（回归）或投票（分类）。最典型的代表是随机森林。
    - 随机森林在Bagging的基础上，还对特征进行随机选择，进一步增加了模型的多样性，抗过拟合能力非常强。
- Boosting：如AdaBoost、GBDT、XGBoost、LightGBM等。这些方法通过顺序训练多个弱学习器来纠正前一个学习器的错误。虽然其本身可能过拟合，但通过控制迭代次数（树的棵数）、学习率、子采样率等参数可以有效防止。

#### 五、 其他高级技术
- 标签平滑：将硬标签（如0或1）转换为软标签（如0.1或0.9），可以减轻模型对训练标签的过度自信，常用于分类任务。
- 对抗训练：在训练数据中加入微小的、人类难以察觉的扰动（对抗样本），让模型对这些扰动变得鲁棒，也能提升泛化能力。

#### 实践建议总结
- 从数据开始：首先尝试获取更多数据或进行数据增强，这是最根本的。
- 使用验证集和提前停止：这是训练神经网络时的标准配置，简单有效。
- 结合正则化与Dropout：在神经网络中，将L2正则化（weight_decay）和Dropout结合使用是常见做法。
- 控制模型复杂度：根据任务难度选择合适的模型容量，不要一味追求复杂模型。
- 集成学习：对于树模型，使用随机森林或通过调整XGBoost/LightGBM的参数（如 max_depth, subsample, colsample_bytree, eta）来防止过拟合。

**核心思想**：在模型的拟合能力（在训练集上表现好）和泛化能力（在测试集上表现好）之间找到一个最佳平衡点。没有一种方法是万能的，通常需要根据具体问题和数据特点，组合使用多种技术。


## 全连接层FC

| 特性 | 说明 |
| :--- | :--- |
| **本质** | 矩阵乘法 + 偏置 + 非线性激活函数 |
| **核心思想** | **整合全部输入信息**，学习特征间的全局交互模式 |
| **最大问题** | **参数量巨大**，尤其当输入维度高时 |
| **主要应用场景** | 1. MLP（全连接网络本身）<br>2. CNN末尾的分类器<br>3. Transformer中的前馈网络<br>4. 各种网络的输出层 |


## 卷积层

**卷积运算**是一种特殊的、带权重的滑动窗口操作。它通过一个称为“卷积核”或“过滤器”的小窗口，在输入数据上滑动，并在每个位置进行局部特征的加权求和。

#### 数学表示

输出[y, x] = Σ_i Σ_j (输入[y+i, x+j] × 核[i, j]) + 偏置

#### 关键参数
| 参数 | 说明 | 常见值/类型 |
|------|------|------------|
| **核尺寸** | 卷积核的大小 | 3×3, 5×5, 7×7 |
| **步长** | 滑动移动的像素数 | 1（密集）, 2（下采样） |
| **填充** | 输入边缘的扩展方式 | `valid`（不填充）, `same`（保持尺寸） |
| **输入通道** | 输入数据的深度 | 1（灰度图）, 3（RGB图） |
| **输出通道** | 卷积核的数量 | 16, 32, 64, 128... |

#### 卷积的核心优势
1. **参数共享**
   - 一个卷积核在整个输入上使用相同权重
   - 大幅减少参数量，降低过拟合风险

2. **局部连接**
   - 每个神经元只连接输入的一个小区域
   - 符合生物视觉原理，计算效率高

3. **平移等变性**
   - 物体移动 → 特征响应同步移动
   - 对位置变化有一定鲁棒性


## 池化层

**池化层**是卷积神经网络中的一种下采样层，用于对特征图进行降维压缩，保留关键信息，同时减少计算量和参数数量，增强模型鲁棒性。

#### 直观理解：信息摘要

将特征图的小区域（如2×2）总结为一个代表性值：

- 详细描述 → 每个像素的具体值（卷积层输出）
- 概括描述 → 区域的核心特征（池化层输出）

#### 主要类型

- 最大池化（最常用）

操作：取窗口区域内的最大值 特点：保留最强激活特征，具有平移不变性

```shell
# 示例：2×2最大池化
输入区域：   输出：
[[1, 3],     → max(1,3,2,9) = 9
 [2, 9]]
```

- 平均池化

操作：取窗口区域内的平均值 特点：保留整体背景信息，平滑特征

```shell
# 示例：2×2平均池化
输入区域：   输出：
[[1, 3],     → (1+3+2+9)/4 = 3.75
 [2, 9]]
```

- 全局池化

操作：对整个特征图的每个通道进行池化 特点：将特征图"坍缩"为向量，极大减少参数量


```shell
# 示例：全局平均池化
输入：[H, W, C]的特征图
输出：[1, 1, C]的向量
```

#### 主要作用
- 降维减参
    - 减少特征图尺寸（H, W）
    - 降低后续层计算复杂度
    - 防止过拟合
- 扩大感受野
    - 高层神经元能"看到"更大输入区域
    - 帮助组合低级特征形成高级特征
- 引入不变性
    - 平移不变性：目标微小平移不影响输出
    - 对微小旋转、缩放有一定鲁棒性
- 特征抽象
    - 过滤噪声和不重要细节
    - 增强特征鲁棒性





